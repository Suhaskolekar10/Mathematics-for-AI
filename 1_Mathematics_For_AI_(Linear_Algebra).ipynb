{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Linear Algebra: Scalars & Vectors**\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 1. **What is a Scalar?**\n",
        "\n",
        "* A **scalar** is a single number (value) representing **magnitude only**, with no direction.\n",
        "* It can be an **integer**, **real number**, or **complex number**.\n",
        "\n",
        "### üî¢ Examples:\n",
        "\n",
        "* 5, -3.2, 0, œÄ\n",
        "* In Python: `a = 5` is a scalar.\n",
        "\n",
        "### üìå **Notation**:\n",
        "\n",
        "* Usually denoted as: lowercase letters like `a`, `b`, `Œ±`, `Œ≤`.\n",
        "\n",
        "### ü§ñ **In AI/ML:**\n",
        "\n",
        "* Scalars appear as:\n",
        "\n",
        "  * **Learning rate (Œ±)** in gradient descent.\n",
        "  * **Loss value** after a forward pass in training.\n",
        "  * **Single pixel intensity** in grayscale images.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 2. **What is a Vector?**\n",
        "\n",
        "A **vector** is an **ordered list of numbers** (scalars) that has both:\n",
        "\n",
        "* **Magnitude**\n",
        "* **Direction**\n",
        "\n",
        "### üî¢ Example:\n",
        "\n",
        "A vector `v = [3, 4]` in 2D space\n",
        "A vector `x = [5.2, -1.7, 3.0]` in 3D space\n",
        "\n",
        "### üìå **Notation**:\n",
        "\n",
        "* Bold lowercase: **v**, **x**, or with arrow: ‚Üíùë£\n",
        "* In programming (Python/NumPy): `v = np.array([3, 4])`\n",
        "\n",
        "### üß† **Intuition**:\n",
        "\n",
        "* A vector is like an arrow pointing from origin to a coordinate point.\n",
        "* In 2D/3D: easy to visualize.\n",
        "* In higher dimensions (e.g., 300D word embeddings), it‚Äôs still a direction in space, just not visually intuitive.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 3. **Types of Vectors**:\n",
        "\n",
        "| Type               | Description                     | Example           |\n",
        "| ------------------ | ------------------------------- | ----------------- |\n",
        "| **Row Vector**     | 1 √ó n matrix                    | `[1, 2, 3]`       |\n",
        "| **Column Vector**  | n √ó 1 matrix                    | `[[1], [2], [3]]` |\n",
        "| **Zero Vector**    | All elements are 0              | `[0, 0, 0]`       |\n",
        "| **Unit Vector**    | Magnitude = 1                   | `[1/‚àö2, 1/‚àö2]`    |\n",
        "| **One-Hot Vector** | Only one element is 1, others 0 | `[0, 0, 1, 0]`    |\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 4. **Key Operations on Vectors**\n",
        "\n",
        "| Operation                 | Formula          | Meaning                                   |   |   |              |                      |\n",
        "| ------------------------- | ---------------- | ----------------------------------------- | - | - | ------------ | -------------------- |\n",
        "| **Addition**              | `a + b`          | Add elements position-wise                |   |   |              |                      |\n",
        "| **Scalar Multiplication** | `k * v`          | Multiply each element by a scalar         |   |   |              |                      |\n",
        "| **Dot Product**           | `a ‚Ä¢ b = Œ£ a·µ¢b·µ¢` | Cosine similarity (magnitude + direction) |   |   |              |                      |\n",
        "| **Norm (Magnitude)**      | v = ‚àö(Œ£ v·µ¢¬≤) | Length of the vector |\n",
        "\n",
        "### ‚ú≥Ô∏è Example:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "v = np.array([3, 4])\n",
        "magnitude = np.linalg.norm(v)  # ‚Üí 5.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 5. **Real Use-Cases of Vectors in AI**\n",
        "\n",
        "| Domain                     | How Vectors Are Used                                             |\n",
        "| -------------------------- | ---------------------------------------------------------------- |\n",
        "| **NLP**                    | Word embeddings (e.g., Word2Vec turns \"king\" into a 300D vector) |\n",
        "| **Computer Vision**        | Flattened image pixels as high-dimensional vectors               |\n",
        "| **Recommendation Systems** | User/item preferences as feature vectors                         |\n",
        "| **Optimization**           | Gradients are vectors used to update model weights               |\n",
        "| **Clustering (ML)**        | Each data point is a vector in feature space                     |\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 6. **Short Tricks / Memory Hacks**\n",
        "\n",
        "* ‚úÖ Remember: **scalars** = size only, **vectors** = size + direction\n",
        "* ‚úÖ Dot product tells **how aligned** two vectors are.\n",
        "* ‚úÖ A unit vector just tells you the **pure direction**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå **Summary (Flash Notes)**\n",
        "\n",
        "| Concept    | Scalar              | Vector                          |\n",
        "| ---------- | ------------------- | ------------------------------- |\n",
        "| Definition | Single value        | List of values                  |\n",
        "| Direction  | ‚ùå No                | ‚úÖ Yes                           |\n",
        "| Visual     | Point               | Arrow                           |\n",
        "| Notation   | `a`, `Œ±`            | `v`, `‚Üív`, bold lowercase       |\n",
        "| Used In    | Learning rate, loss | Features, embeddings, gradients |\n",
        "| Examples   | `5`, `œÄ`            | `[2, -1]`, `[1, 0, 0]`          |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **2. Linear Algebra: Norm, Vector Space, Cosine Similarity & Basic Terms**\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 1. **Norm of a Vector**\n",
        "\n",
        "The **norm** of a vector is a measure of its **length or magnitude**. It tells you **how far** the vector is from the origin in space.\n",
        "\n",
        "### üìå **Common Types of Norms**:\n",
        "\n",
        "| Type                    | Formula           | Description                    | Use in AI              |                                     |                                 |\n",
        "| ----------------------- | ----------------- | ------------------------------ | ---------------------- | ----------------------------------- | ------------------------------- |\n",
        "| **L1 Norm (Manhattan)** | `‚Äñv‚Äñ‚ÇÅ = v·µ¢'    | Sum of absolute values              | Sparsity (Lasso Regularization) |\n",
        "| **L2 Norm (Euclidean)** | `‚Äñv‚Äñ‚ÇÇ = ‚àö(Œ£ v·µ¢¬≤)` | Euclidean distance from origin | Most common in ML & DL |                                     |                                 |\n",
        "| **Infinity Norm (Max)** | `‚Äñv‚Äñ‚àû = max(v·µ¢)' | Maximum value among vector elements | Rare, used in adversarial ML    |\n",
        "\n",
        "### üß† **Example**:\n",
        "\n",
        "Let `v = [3, 4]`\n",
        "\n",
        "* L2 norm: `‚Äñv‚Äñ‚ÇÇ = ‚àö(3¬≤ + 4¬≤) = ‚àö25 = 5`\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "v = np.array([3, 4])\n",
        "np.linalg.norm(v)  # Output: 5.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 2. **Unit Vector**\n",
        "\n",
        "A **unit vector** has **magnitude 1** and only gives **direction**.\n",
        "\n",
        "### ‚ú≥Ô∏è Formula:\n",
        "\n",
        "$$\n",
        "\\hat{v} = \\frac{v}{‚Äñv‚Äñ}\n",
        "$$\n",
        "\n",
        "### üìå In AI:\n",
        "\n",
        "Used to normalize features (e.g., during cosine similarity), helps in **standardizing input**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 3. **Vector Space**\n",
        "\n",
        "A **vector space** is a collection of vectors that can be:\n",
        "\n",
        "* Added together\n",
        "* Multiplied by scalars\n",
        "  ...and still remain within the space.\n",
        "\n",
        "### üß† **Example**:\n",
        "\n",
        "The 2D plane `‚Ñù¬≤` is a vector space. If `v‚ÇÅ = [1, 2]` and `v‚ÇÇ = [3, 4]`, then `v‚ÇÅ + v‚ÇÇ = [4, 6]` is also in `‚Ñù¬≤`.\n",
        "\n",
        "### ‚úÖ **Properties of a Vector Space**:\n",
        "\n",
        "* Closure under addition & scalar multiplication\n",
        "* Existence of zero vector\n",
        "* Existence of additive inverse\n",
        "* Distributive & associative laws\n",
        "\n",
        "### ü§ñ **In AI:**\n",
        "\n",
        "* **Word embeddings** live in a high-dimensional vector space.\n",
        "* **Linear models** (like SVM, logistic regression) operate within vector spaces.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 4. **Cosine Similarity**\n",
        "\n",
        "Measures the **angle** between two vectors, not the magnitude.\n",
        "\n",
        "### ‚ú≥Ô∏è Formula:\n",
        "\n",
        "$$\n",
        "\\cos(Œ∏) = \\frac{A \\cdot B}{‚ÄñA‚Äñ ‚ÄñB‚Äñ}\n",
        "$$\n",
        "\n",
        "* Ranges from `-1` (opposite direction) to `1` (same direction)\n",
        "* If vectors point in the **same direction**, cosine similarity = 1\n",
        "\n",
        "### üß† **Why use Cosine Similarity?**\n",
        "\n",
        "* When **magnitude doesn‚Äôt matter**, only direction (e.g., text data, embeddings)\n",
        "* More robust than Euclidean distance in high-dimensional data\n",
        "\n",
        "```python\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2]])\n",
        "B = np.array([[2, 4]])\n",
        "cosine_similarity(A, B)  # Output: [[1.0]]\n",
        "```\n",
        "\n",
        "### ü§ñ **In AI:**\n",
        "\n",
        "| Application    | Use                                     |\n",
        "| -------------- | --------------------------------------- |\n",
        "| NLP            | Compare word embeddings                 |\n",
        "| Recommendation | Item/user similarity                    |\n",
        "| Clustering     | Distance metric between feature vectors |\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 5. **Orthogonality**\n",
        "\n",
        "Two vectors are **orthogonal** if their **dot product is 0**, meaning they are **perpendicular** and **independent**.\n",
        "\n",
        "### ‚ú≥Ô∏è Dot Product:\n",
        "\n",
        "$$\n",
        "a \\cdot b = 0 \\Rightarrow \\text{Vectors are orthogonal}\n",
        "$$\n",
        "\n",
        "### ü§ñ In AI:\n",
        "\n",
        "* **Orthogonal vectors** ‚Üí uncorrelated features.\n",
        "* Helps in **feature selection** and reducing **multicollinearity**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 6. **Span**\n",
        "\n",
        "The **span** of a set of vectors is the **set of all vectors** that can be formed by their **linear combinations**.\n",
        "\n",
        "### üß† Example:\n",
        "\n",
        "If vectors `v1 = [1, 0]`, `v2 = [0, 1]` then their span covers all of ‚Ñù¬≤.\n",
        "\n",
        "### ü§ñ In AI:\n",
        "\n",
        "* Determines the **expressiveness** of the feature space\n",
        "* **Basis vectors** span a space ‚Üí used in dimensionality reduction (PCA)\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 7. **Linear Independence**\n",
        "\n",
        "A set of vectors is **linearly independent** if **none** of them can be written as a **linear combination** of the others.\n",
        "\n",
        "### ü§ñ In AI:\n",
        "\n",
        "* Ensures that features provide **unique** information\n",
        "* Basis vectors in PCA are linearly independent\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Quick Summary Table:\n",
        "\n",
        "| Term                    | Meaning                                                    | Use in AI                               |\n",
        "| ----------------------- | ---------------------------------------------------------- | --------------------------------------- |\n",
        "| **Norm**                | Length of a vector                                         | Feature normalization, gradient control |\n",
        "| **Unit Vector**         | Vector with magnitude 1                                    | Direction only, normalization           |\n",
        "| **Vector Space**        | Set of vectors closed under addition/scalar multiplication | Feature representation, embeddings      |\n",
        "| **Cosine Similarity**   | Angle between vectors                                      | Similarity in NLP, recsys               |\n",
        "| **Orthogonality**       | Perpendicular vectors (dot = 0)                            | Feature independence                    |\n",
        "| **Span**                | All vectors formed from combinations                       | Coverage of feature space               |\n",
        "| **Linear Independence** | No vector depends on others                                | Avoid redundancy in features            |\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Use-Cases Recap:\n",
        "\n",
        "| Concept             | Real Use                                                          |\n",
        "| ------------------- | ----------------------------------------------------------------- |\n",
        "| Norms               | Control size of gradients in training (avoid exploding gradients) |\n",
        "| Cosine Similarity   | Search engines, sentence similarity, recommendation engines       |\n",
        "| Vector Space        | Word embeddings (GloVe, BERT), Feature vectors                    |\n",
        "| Orthogonality       | Ensure non-overlapping features                                   |\n",
        "| Linear Independence | Dimensionality reduction, noise removal                           |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **3. Linear Algebra: Dot Product and Projections**\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 1. **Dot Product (Scalar Product)**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "The **dot product** of two vectors results in a **scalar** and tells how much two vectors **align** with each other.\n",
        "\n",
        "### ‚ú≥Ô∏è **Formula (Algebraic)**:\n",
        "\n",
        "For two vectors `A = [a‚ÇÅ, a‚ÇÇ, ..., a‚Çô]`, `B = [b‚ÇÅ, b‚ÇÇ, ..., b‚Çô]`:\n",
        "\n",
        "$$\n",
        "A \\cdot B = a‚ÇÅb‚ÇÅ + a‚ÇÇb‚ÇÇ + ... + a‚Çôb‚Çô = \\sum_{i=1}^{n} a_i b_i\n",
        "$$\n",
        "\n",
        "### ‚ú≥Ô∏è **Formula (Geometric)**:\n",
        "\n",
        "$$\n",
        "A \\cdot B = ‚ÄñA‚Äñ‚ÄñB‚Äñ \\cos(Œ∏)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* `‚ÄñA‚Äñ` = magnitude (norm) of vector A\n",
        "* `Œ∏` = angle between A and B\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Interpretation**:\n",
        "\n",
        "* If `A ‚Ä¢ B > 0`: angle < 90¬∞, vectors point in **same direction**\n",
        "* If `A ‚Ä¢ B = 0`: vectors are **orthogonal (perpendicular)**\n",
        "* If `A ‚Ä¢ B < 0`: angle > 90¬∞, vectors point in **opposite directions**\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Example**:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "a = np.array([2, 3])\n",
        "b = np.array([4, 1])\n",
        "dot = np.dot(a, b)  # Output: 11\n",
        "```\n",
        "\n",
        "$$\n",
        "A \\cdot B = 2√ó4 + 3√ó1 = 8 + 3 = 11\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ **Applications in AI/ML**:\n",
        "\n",
        "| Use Case                 | Description                                                |\n",
        "| ------------------------ | ---------------------------------------------------------- |\n",
        "| **Cosine Similarity**    | Normalized dot product tells **text/vector similarity**    |\n",
        "| **Attention Mechanisms** | Query ‚Ä¢ Key ‚Üí Attention score (Transformer, GPT)           |\n",
        "| **Neural Networks**      | Neuron output: `z = w ‚Ä¢ x + b`                             |\n",
        "| **Loss Gradients**       | Backprop uses dot product between error and weight vectors |\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 2. **Projection of a Vector**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "The **projection** of vector `A` onto vector `B` is the **shadow of A** in the direction of B.\n",
        "\n",
        "### ‚ú≥Ô∏è **Formula (scalar projection)**:\n",
        "\n",
        "$$\n",
        "\\text{proj}_{B}(A) = \\frac{A \\cdot B}{‚ÄñB‚Äñ}\n",
        "$$\n",
        "\n",
        "### ‚ú≥Ô∏è **Formula (vector projection)**:\n",
        "\n",
        "$$\n",
        "\\vec{\\text{proj}}_{B}(A) = \\left( \\frac{A \\cdot B}{‚ÄñB‚Äñ^2} \\right) B\n",
        "$$\n",
        "\n",
        "* It gives a new vector **in the direction of B**, but scaled to how much A lies in that direction.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Geometric Intuition**:\n",
        "\n",
        "Imagine shining a light on A ‚Äî its shadow on B is the **projection**.\n",
        "\n",
        "* If A and B are aligned ‚Üí projection is full magnitude.\n",
        "* If A and B are orthogonal ‚Üí projection is 0.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Example**:\n",
        "\n",
        "Let `A = [3, 4]` and `B = [1, 0]`\n",
        "\n",
        "```python\n",
        "A = np.array([3, 4])\n",
        "B = np.array([1, 0])\n",
        "scalar_proj = np.dot(A, B) / np.linalg.norm(B)  # Output: 3\n",
        "vector_proj = (np.dot(A, B) / np.dot(B, B)) * B  # Output: [3, 0]\n",
        "```\n",
        "\n",
        "The vector `[3, 0]` is A's projection onto B.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ **Applications in AI/ML**:\n",
        "\n",
        "| Use Case                               | Description                                                                |\n",
        "| -------------------------------------- | -------------------------------------------------------------------------- |\n",
        "| **Gradient Descent**                   | Gradients are projected directions for parameter updates                   |\n",
        "| **PCA (Principal Component Analysis)** | Projects high-dimensional data onto **principal axes**                     |\n",
        "| **Word Embeddings**                    | Project one word vector onto another to measure **semantic relationships** |\n",
        "| **Feature Extraction**                 | Useful in dimensionality reduction and **noise filtering**                 |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary Table\n",
        "\n",
        "| Concept               | Formula              | Output | Intuition          | AI Use                  |\n",
        "| --------------------- | -------------------- | ------ | ------------------ | ----------------------- |\n",
        "| **Dot Product**       | `A ‚Ä¢ B = Œ£ a·µ¢b·µ¢`     | Scalar | Measures alignment | Neural nets, similarity |\n",
        "| **Scalar Projection** | `(A ‚Ä¢ B)/‚ÄñB‚Äñ`        | Scalar | Shadow length      | Similarity, projection  |\n",
        "| **Vector Projection** | `((A ‚Ä¢ B)/‚ÄñB‚Äñ¬≤) √ó B` | Vector | Shadow vector      | PCA, attention          |\n",
        "| **Orthogonal**        | `A ‚Ä¢ B = 0`          | -      | Independent        | Feature design          |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úçÔ∏è Bonus: Visualization Tip\n",
        "\n",
        "* Think of **dot product** as asking: *‚ÄúHow much of A goes in B‚Äôs direction?‚Äù*\n",
        "* Think of **projection** as: *‚ÄúLet‚Äôs find the actual portion of A that lies on B.‚Äù*\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **4. Linear Algebra: Matrices and Matrix Operations**\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 1. **What is a Matrix?**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "A **matrix** is a **2D rectangular array** of numbers, arranged in **rows** and **columns**.\n",
        "\n",
        "If a matrix has `m` rows and `n` columns, we say it's of **dimension** `m √ó n`.\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix} \\quad (2 √ó 3 \\text{ matrix})\n",
        "$$\n",
        "\n",
        "### üìå **Terminology**:\n",
        "\n",
        "* **Element** at row `i`, column `j` is written as `a·µ¢‚±º`\n",
        "* **Square matrix**: Rows = Columns (`n √ó n`)\n",
        "* **Column vector**: `n √ó 1` matrix\n",
        "* **Row vector**: `1 √ó n` matrix\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 2. **Basic Matrix Operations**\n",
        "\n",
        "---\n",
        "\n",
        "### üìò A. **Matrix Addition / Subtraction**\n",
        "\n",
        "* Only possible if matrices are of **same size**\n",
        "\n",
        "$$\n",
        "A + B = [a·µ¢‚±º + b·µ¢‚±º]\n",
        "$$\n",
        "\n",
        "### üîç AI Use:\n",
        "\n",
        "* Combining outputs from layers\n",
        "* Adding bias terms to weight matrices\n",
        "\n",
        "---\n",
        "\n",
        "### üìò B. **Scalar Multiplication**\n",
        "\n",
        "* Multiply **each element** by a scalar\n",
        "\n",
        "$$\n",
        "kA = [k √ó a·µ¢‚±º]\n",
        "$$\n",
        "\n",
        "### üîç AI Use:\n",
        "\n",
        "* Scaling features or learning rates\n",
        "\n",
        "---\n",
        "\n",
        "### üìò C. **Matrix Multiplication**\n",
        "\n",
        "* A `m √ó n` matrix `A` can be multiplied by a `n √ó p` matrix `B`\n",
        "* Result: `m √ó p` matrix\n",
        "\n",
        "$$\n",
        "C = A √ó B \\quad \\text{where } c_{ij} = \\sum_{k=1}^{n} a_{ik} √ó b_{kj}\n",
        "$$\n",
        "\n",
        "> Not **element-wise**, this is **dot product** between row of A and column of B.\n",
        "\n",
        "### üîç AI Use:\n",
        "\n",
        "* **Core of neural network forward pass**:\n",
        "\n",
        "  * `Z = W √ó X + b` where:\n",
        "\n",
        "    * `X` is input matrix\n",
        "    * `W` is weight matrix\n",
        "    * `Z` is output (logits)\n",
        "\n",
        "---\n",
        "\n",
        "### üìò D. **Transpose of a Matrix (A·µÄ)**\n",
        "\n",
        "* Flip matrix over its diagonal\n",
        "\n",
        "$$\n",
        "A·µÄ_{ij} = A_{ji}\n",
        "$$\n",
        "\n",
        "If\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}1 & 2\\\\ 3 & 4\\end{bmatrix} \\quad ‚áí \\quad A·µÄ = \\begin{bmatrix}1 & 3\\\\ 2 & 4\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### üîç AI Use:\n",
        "\n",
        "* Used in **vector-matrix operations**\n",
        "* Helps align dimensions during dot products\n",
        "* Transposing weight matrices in backpropagation\n",
        "\n",
        "---\n",
        "\n",
        "### üìò E. **Identity Matrix (I)**\n",
        "\n",
        "* Square matrix with `1s` on diagonal and `0s` elsewhere\n",
        "\n",
        "$$\n",
        "I = \\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "AI = IA = A\n",
        "$$\n",
        "\n",
        "### üîç AI Use:\n",
        "\n",
        "* Acts like **1** for matrix multiplication\n",
        "* Used in **initialization**, **linear transformations**, and **solving equations**\n",
        "\n",
        "---\n",
        "\n",
        "### üìò F. **Inverse Matrix (A‚Åª¬π)**\n",
        "\n",
        "* For square matrix A, if:\n",
        "\n",
        "$$\n",
        "AA^{-1} = A^{-1}A = I\n",
        "$$\n",
        "\n",
        "...then `A‚Åª¬π` is the inverse.\n",
        "\n",
        "> Only **non-singular** matrices (det(A) ‚â† 0) have inverses.\n",
        "\n",
        "### üîç AI Use:\n",
        "\n",
        "* Solving equations like `Ax = b` ‚Üí `x = A‚Åª¬πb`\n",
        "* In practice, avoided due to instability; **pseudo-inverse or numerical solvers** are used instead\n",
        "\n",
        "---\n",
        "\n",
        "### üìò G. **Element-wise Operations**\n",
        "\n",
        "Sometimes denoted with `‚äô` (Hadamard Product):\n",
        "\n",
        "* Multiply corresponding elements of same-sized matrices\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[10, 20], [30, 40]])\n",
        "C = A * B  # Element-wise multiplication\n",
        "```\n",
        "\n",
        "### üîç AI Use:\n",
        "\n",
        "* Activation functions applied element-wise\n",
        "* Gate computations in **LSTMs**, **attention**\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 3. **Matrix Properties (Quick Table)**\n",
        "\n",
        "| Property                 | Description        |\n",
        "| ------------------------ | ------------------ |\n",
        "| **Associativity**        | A(BC) = (AB)C      |\n",
        "| **Distributivity**       | A(B + C) = AB + AC |\n",
        "| **Non-Commutative**      | AB ‚â† BA (usually)  |\n",
        "| **Transpose of Product** | (AB)·µÄ = B·µÄA·µÄ       |\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 4. **Matrix in AI ‚Äî Real Examples**\n",
        "\n",
        "| Concept        | Matrix Example                             | AI Usage                |\n",
        "| -------------- | ------------------------------------------ | ----------------------- |\n",
        "| **Inputs**     | Image: `28x28` ‚Üí `784x1`                   | CNNs, input layers      |\n",
        "| **Weights**    | Hidden layer weights: `n_hidden √ó n_input` | Feedforward computation |\n",
        "| **Batch Data** | Batch of 64 inputs ‚Üí `64 √ó input_dim`      | Training in batches     |\n",
        "| **Embeddings** | Word matrix: `vocab_size √ó embedding_dim`  | NLP                     |\n",
        "| **Attention**  | Matrices: Query √ó Key = Score              | Transformers, GPT       |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary Flash Notes\n",
        "\n",
        "| Operation | Meaning            | AI/ML Use                     |\n",
        "| --------- | ------------------ | ----------------------------- |\n",
        "| `A + B`   | Add element-wise   | Combine features              |\n",
        "| `kA`      | Scalar multiply    | Scale features                |\n",
        "| `AB`      | Matrix multiply    | Neural net forward pass       |\n",
        "| `A·µÄ`      | Transpose          | Adjust shape for dot product  |\n",
        "| `A‚Åª¬π`     | Inverse            | Solve equations (theoretical) |\n",
        "| `I`       | Identity           | Neutral element               |\n",
        "| `A ‚äô B`   | Element-wise mult. | Activation, attention gates   |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úçÔ∏è Visual Intuition:\n",
        "\n",
        "* Think of matrices as **data transformers** ‚Äî each operation transforms input vectors to new representations.\n",
        "* In neural networks: matrices **rotate, stretch, and compress** data through layers.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **5.Linear Algebra: Determinant and Rank**\n",
        "\n",
        "*(With AI/ML Applications)*\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 1. **Determinant of a Matrix**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "The **determinant** is a **scalar value** that summarizes certain properties of a square matrix.\n",
        "\n",
        "For a square matrix `A`, the determinant is denoted as `det(A)` or `|A|`.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ **For Small Matrices**:\n",
        "\n",
        "#### üîπ 2√ó2 Matrix:\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{bmatrix}\n",
        "\\Rightarrow \\text{det}(A) = ad - bc\n",
        "$$\n",
        "\n",
        "#### üîπ 3√ó3 Matrix:\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "a & b & c \\\\\n",
        "d & e & f \\\\\n",
        "g & h & i\n",
        "\\end{bmatrix}\n",
        "\\Rightarrow \\text{det}(A) = a(ei - fh) - b(di - fg) + c(dh - eg)\n",
        "$$\n",
        "\n",
        "For larger matrices, we use **cofactor expansion** or **LU decomposition**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Geometric Meaning**:\n",
        "\n",
        "* **2D**: Area of the parallelogram formed by 2 column vectors\n",
        "* **3D**: Volume of the parallelepiped\n",
        "* **Higher Dimensions**: Generalizes to hypervolume\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Key Properties**:\n",
        "\n",
        "| Property                              | Implication                            |\n",
        "| ------------------------------------- | -------------------------------------- |\n",
        "| `det(A) = 0`                          | Matrix is **singular**, not invertible |\n",
        "| `det(AB) = det(A) √ó det(B)`           | Multiplicative                         |\n",
        "| `det(A·µÄ) = det(A)`                    | Transpose doesn't change determinant   |\n",
        "| Swapping rows ‚áí Changes sign of `det` | Useful in LU-based computations        |\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ **Use in AI/ML**:\n",
        "\n",
        "| Application                      | Description                                                              |\n",
        "| -------------------------------- | ------------------------------------------------------------------------ |\n",
        "| **Model Invertibility**          | `det(A) = 0` means we **can‚Äôt solve** linear systems reliably            |\n",
        "| **Feature Space Transformation** | Measures change in volume under transformation                           |\n",
        "| **Jacobian Determinant**         | In generative models (e.g., **normalizing flows**) to compute likelihood |\n",
        "| **Stability Checks**             | In optimization algorithms & numerical solvers                           |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú≥Ô∏è **Python Example**:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "det = np.linalg.det(A)  # Output: -2.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 2. **Rank of a Matrix**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "The **rank** of a matrix is the **maximum number of linearly independent rows or columns**.\n",
        "\n",
        "* Denoted as `rank(A)`\n",
        "* Rank ‚â§ min(rows, columns)\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **What Does Rank Tell Us?**\n",
        "\n",
        "| Rank             | Meaning                                               |\n",
        "| ---------------- | ----------------------------------------------------- |\n",
        "| Full Rank        | All rows/columns are linearly independent             |\n",
        "| Rank < min(m, n) | Some rows/columns are linear combinations (redundant) |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Geometric Interpretation**:\n",
        "\n",
        "* Rank tells us the **dimension of the space spanned** by the matrix (called the **column space**).\n",
        "* For instance, a 3D dataset lying on a 2D plane ‚Üí rank = 2.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Key Properties**:\n",
        "\n",
        "| Property                           | Description                               |\n",
        "| ---------------------------------- | ----------------------------------------- |\n",
        "| `rank(A) = rank(A·µÄ)`               | Symmetry                                  |\n",
        "| `rank(AB) ‚â§ min(rank(A), rank(B))` | Multiplication effect                     |\n",
        "| Full rank matrix has inverse       | If square and `rank = n`, then invertible |\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ **Use in AI/ML**:\n",
        "\n",
        "| Application                        | Description                                                        |\n",
        "| ---------------------------------- | ------------------------------------------------------------------ |\n",
        "| **PCA (Dimensionality Reduction)** | Rank = number of meaningful dimensions                             |\n",
        "| **Overfitting Detection**          | Low-rank data suggests **feature redundancy**                      |\n",
        "| **Linear Regression**              | Design matrix `X` must be full rank to **uniquely solve** `Xw = y` |\n",
        "| **Data Compression**               | Low-rank approximations to reduce storage (SVD, autoencoders)      |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú≥Ô∏è **Python Example**:\n",
        "\n",
        "```python\n",
        "A = np.array([[1, 2], [2, 4]])\n",
        "rank = np.linalg.matrix_rank(A)  # Output: 1 (because second row is twice the first)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary Table\n",
        "\n",
        "| Concept         | Formula   | Tells You                          | AI Use                                       |\n",
        "| --------------- | --------- | ---------------------------------- | -------------------------------------------- |\n",
        "| **Determinant** | `det(A)`  | Volume scale factor; invertibility | Generative models, numerical stability       |\n",
        "| **Rank**        | `rank(A)` | # of independent directions        | PCA, redundancy detection, model solvability |\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Real-World AI Examples\n",
        "\n",
        "| Task                  | Matrix Use                                                      |\n",
        "| --------------------- | --------------------------------------------------------------- |\n",
        "| **PCA**               | Choose `k` largest eigenvectors ‚Üí rank defines how many to keep |\n",
        "| **Linear Regression** | Solve `Xw = y` only if `rank(X) = #features`                    |\n",
        "| **Autoencoders**      | Compress to a low-rank latent space                             |\n",
        "| **Text Embeddings**   | Analyze rank to reduce dimensions via SVD or LSA                |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **6. Linear Algebra: Eigenvalues and Eigenvectors**\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 1. **What are Eigenvalues and Eigenvectors?**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "For a **square matrix** `A`, an **eigenvector** `v` is a **non-zero vector** such that when you multiply it by the matrix `A`, the result is a **scaled version** of the same vector `v`.\n",
        "\n",
        "$$\n",
        "A \\cdot v = \\lambda \\cdot v\n",
        "$$\n",
        "\n",
        "* `v`: Eigenvector\n",
        "* `Œª` (lambda): **Eigenvalue**\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Visual Intuition**:\n",
        "\n",
        "Think of a matrix `A` as a **transformation** (like stretching, rotating, scaling).\n",
        "Most vectors change **direction and length** under `A`.\n",
        "But **eigenvectors keep their direction** ‚Äî only **scaled** by Œª (eigenvalue).\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ **Example (2√ó2 Matrix)**:\n",
        "\n",
        "Let:\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix},\\quad\n",
        "v_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix},\\quad\n",
        "v_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "A v_1 = 2v_1,\\quad A v_2 = 3v_2\n",
        "$$\n",
        "\n",
        "‚Üí `v‚ÇÅ` and `v‚ÇÇ` are eigenvectors with eigenvalues 2 and 3, respectively.\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 2. **How to Compute Eigenvalues and Eigenvectors**\n",
        "\n",
        "### üßÆ Step 1: Characteristic Equation\n",
        "\n",
        "$$\n",
        "A \\cdot v = \\lambda \\cdot v \\Rightarrow (A - \\lambda I)v = 0\n",
        "$$\n",
        "\n",
        "To solve:\n",
        "\n",
        "$$\n",
        "\\det(A - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "This gives a polynomial in Œª ‚Üí **characteristic equation**.\n",
        "Roots are the **eigenvalues**.\n",
        "\n",
        "### üßÆ Step 2: Solve for Eigenvectors\n",
        "\n",
        "Once you know Œª, plug into:\n",
        "\n",
        "$$\n",
        "(A - \\lambda I)v = 0\n",
        "$$\n",
        "\n",
        "and solve for non-zero `v`.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú≥Ô∏è **Python Example**:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[2, 1], [1, 2]])\n",
        "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
        "\n",
        "print(\"Eigenvalues:\", eigenvalues)\n",
        "print(\"Eigenvectors:\", eigenvectors)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 3. **Key Properties**\n",
        "\n",
        "| Property                                                  | Description       |\n",
        "| --------------------------------------------------------- | ----------------- |\n",
        "| Only for square matrices                                  | A must be `n √ó n` |\n",
        "| Eigenvectors are non-zero                                 | `v ‚â† 0`           |\n",
        "| Matrix can have ‚â§ n eigenvalues                           | Depending on rank |\n",
        "| Eigenvectors of **symmetric matrices** are **orthogonal** | Useful in PCA     |\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 4. **Use of Eigenvalues and Eigenvectors in AI/ML**\n",
        "\n",
        "### ‚úÖ A. **Principal Component Analysis (PCA)**\n",
        "\n",
        "* PCA finds the **directions of maximum variance** in data.\n",
        "* These directions are the **eigenvectors** of the **covariance matrix**.\n",
        "* The corresponding **eigenvalues** tell how much variance each component explains.\n",
        "\n",
        "$$\n",
        "\\text{Covariance Matrix} = \\frac{1}{n} X^T X\n",
        "\\Rightarrow \\text{Eigenvectors = PCA directions}\n",
        "$$\n",
        "\n",
        "### ‚úÖ B. **Spectral Clustering**\n",
        "\n",
        "* Uses **eigenvectors of graph Laplacians** to cluster nodes in graphs.\n",
        "\n",
        "### ‚úÖ C. **Understanding Model Stability**\n",
        "\n",
        "* In optimization, **eigenvalues of the Hessian matrix** show if a point is a **minima, maxima, or saddle point**.\n",
        "\n",
        "### ‚úÖ D. **Deep Learning (Backpropagation)**\n",
        "\n",
        "* Eigenvalues of **weight matrices** can indicate **vanishing/exploding gradients** ‚Üí helps with stability.\n",
        "\n",
        "### ‚úÖ E. **Quantum Computing, GNNs, LSA, SVD**\n",
        "\n",
        "* All rely heavily on eigen-decomposition concepts.\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 5. **Summary Table**\n",
        "\n",
        "| Concept         | Meaning                                          | AI/ML Use                             |\n",
        "| --------------- | ------------------------------------------------ | ------------------------------------- |\n",
        "| **Eigenvector** | Vector that keeps direction under transformation | PCA directions, stable directions     |\n",
        "| **Eigenvalue**  | Scalar that scales the eigenvector               | Explained variance, gradient strength |\n",
        "| **Symmetric A** | Orthogonal eigenvectors                          | Used in PCA, SVD                      |\n",
        "| **Large Œª**     | Strong variation in that direction               | Keep top Œªs in PCA                    |\n",
        "| **Œª = 0**       | Data lies in a lower dimension                   | Rank deficiency                       |\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 6. **Bonus: PCA Explained via Eigen Concepts**\n",
        "\n",
        "### Let data matrix `X`:\n",
        "\n",
        "* Compute **covariance matrix**: `C = X·µÄX`\n",
        "* Compute **eigenvectors of C** ‚Üí principal directions\n",
        "* Select top `k` eigenvectors with largest eigenvalues\n",
        "* Project data onto those `k` directions ‚Üí reduced dimensional space\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Flash Notes\n",
        "\n",
        "| Term              | Description                          |\n",
        "| ----------------- | ------------------------------------ |\n",
        "| `Av = Œªv`         | Definition of eigenvalue/eigenvector |\n",
        "| `det(A - ŒªI) = 0` | Characteristic equation              |\n",
        "| **Œª > 1**         | Stretching                           |\n",
        "| **Œª = 1**         | No scale change                      |\n",
        "| **Œª < 1**         | Compression                          |\n",
        "| **Œª = 0**         | Collapse (no variance)               |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **7. Linear Algebra: Vector Spaces & Transformations**\n",
        "---\n",
        "\n",
        "## üî∑ 1. **What is a Vector Space?**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "A **vector space** is a set of vectors (real or complex) that is:\n",
        "\n",
        "* **Closed under vector addition**\n",
        "* **Closed under scalar multiplication**\n",
        "\n",
        "That means:\n",
        "If `v‚ÇÅ` and `v‚ÇÇ` are in the vector space **V**, then:\n",
        "\n",
        "* `v‚ÇÅ + v‚ÇÇ` ‚àà V\n",
        "* `a * v‚ÇÅ` ‚àà V for any scalar `a`\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Requirements (Axioms)**:\n",
        "\n",
        "A valid vector space must follow these rules:\n",
        "\n",
        "| Property                            | Example                       |\n",
        "| ----------------------------------- | ----------------------------- |\n",
        "| Closure under addition              | `v + w ‚àà V`                   |\n",
        "| Closure under scalar multiplication | `a * v ‚àà V`                   |\n",
        "| Associativity & commutativity       | `(u + v) + w = u + (v + w)`   |\n",
        "| Zero vector exists                  | `0 ‚àà V` such that `v + 0 = v` |\n",
        "| Every vector has an inverse         | `v + (-v) = 0`                |\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ **Examples of Vector Spaces**:\n",
        "\n",
        "| Space          | Example                                                |\n",
        "| -------------- | ------------------------------------------------------ |\n",
        "| ‚Ñù¬≤ (2D plane)  | `[1, 2]`, `[3, 4]`                                     |\n",
        "| ‚Ñù¬≥ (3D space)  | `[1, 0, -1]`                                           |\n",
        "| ‚Ñù‚Åø             | Word embeddings (300D vectors)                         |\n",
        "| Matrix space   | All `m √ó n` matrices                                   |\n",
        "| Function space | Functions like `f(x) = sin(x)` form vector spaces too! |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Why Are Vector Spaces Important in AI?**\n",
        "\n",
        "They allow us to:\n",
        "\n",
        "* Represent data as geometric objects\n",
        "* Use **linear algebra** to manipulate, reduce, and transform high-dimensional information\n",
        "* Define **direction, distance, and angles** between points (vectors)\n",
        "* Analyze **data structure**, redundancy, and learnable patterns\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 2. **Basis and Dimension**\n",
        "\n",
        "### ‚úÖ **Basis**:\n",
        "\n",
        "A **basis** of a vector space is a **minimal set of linearly independent vectors** that **span** the space.\n",
        "\n",
        "> Think of it as the set of \"building blocks\" needed to reconstruct any vector in the space.\n",
        "\n",
        "### ‚úÖ **Dimension**:\n",
        "\n",
        "The number of basis vectors ‚Üí **dimension** of the space.\n",
        "\n",
        "### üî¢ Examples:\n",
        "\n",
        "* Basis of ‚Ñù¬≤: `[(1,0), (0,1)]` ‚Üí 2D\n",
        "* Word embeddings: 300 basis vectors ‚Üí 300D space\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 3. **Linear Transformation (Mapping Vectors)**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "A **linear transformation** `T` maps vectors from one space to another:\n",
        "\n",
        "$$\n",
        "T: V ‚Üí W\n",
        "$$\n",
        "\n",
        "It satisfies:\n",
        "\n",
        "* **T(v + w) = T(v) + T(w)**\n",
        "* **T(c \\* v) = c \\* T(v)**\n",
        "\n",
        "> In matrix form:\n",
        "\n",
        "$$\n",
        "T(v) = A \\cdot v\n",
        "$$\n",
        "\n",
        "Where `A` is a matrix representing the transformation.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Visual Intuition**:\n",
        "\n",
        "Think of a matrix as:\n",
        "\n",
        "* Rotating vectors\n",
        "* Stretching/shrinking them\n",
        "* Projecting them onto lower dimensions\n",
        "* Reflecting or flipping them\n",
        "\n",
        "Each transformation reshapes the vector space, and this is how neural networks **learn**.\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ Examples:\n",
        "\n",
        "#### üîπ Rotation Matrix (2D):\n",
        "\n",
        "$$\n",
        "R = \\begin{bmatrix}\n",
        "\\cos Œ∏ & -\\sin Œ∏ \\\\\n",
        "\\sin Œ∏ & \\cos Œ∏\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### üîπ Scaling Matrix:\n",
        "\n",
        "$$\n",
        "S = \\begin{bmatrix}\n",
        "2 & 0 \\\\\n",
        "0 & 3\n",
        "\\end{bmatrix}\n",
        "\\Rightarrow \\text{Scales x by 2 and y by 3}\n",
        "$$\n",
        "\n",
        "#### üîπ Projection Matrix:\n",
        "\n",
        "$$\n",
        "P = \\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{bmatrix}\n",
        "\\Rightarrow \\text{Projects vector onto the x-axis}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 4. **Kernel and Image**\n",
        "\n",
        "| Term                    | Meaning                                     |\n",
        "| ----------------------- | ------------------------------------------- |\n",
        "| **Kernel (Null Space)** | Set of all vectors `v` such that `T(v) = 0` |\n",
        "| **Image (Range)**       | Set of all output vectors `T(v)`            |\n",
        "\n",
        "* If the **kernel is non-zero**, the transformation **loses information** (used in detecting **dimensionality reduction**).\n",
        "* Full-rank transformations have only the **zero vector in the kernel**.\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 5. **AI & ML Applications of Vector Spaces & Transformations**\n",
        "\n",
        "| Area                      | Use of Vector Spaces/Transformations                                                        |\n",
        "| ------------------------- | ------------------------------------------------------------------------------------------- |\n",
        "| **PCA**                   | Transforms high-D data to lower-D using eigenvectors                                        |\n",
        "| **Neural Networks**       | Each layer applies a **linear transformation** followed by non-linearity                    |\n",
        "| **Word Embeddings**       | Words live in a vector space; directions capture meaning (e.g., king - man + woman ‚âà queen) |\n",
        "| **Autoencoders**          | Learn a compressed subspace that represents the data                                        |\n",
        "| **GANs**                  | Latent vectors lie in a learned vector space, transformed into images                       |\n",
        "| **Graph Neural Networks** | Project graph nodes into new vector spaces at each layer                                    |\n",
        "| **Transformer Attention** | Query, Key, Value vectors are linearly transformed for attention calculation                |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Flash Summary\n",
        "\n",
        "| Concept                   | Definition                                            | Use in AI                   |\n",
        "| ------------------------- | ----------------------------------------------------- | --------------------------- |\n",
        "| **Vector Space**          | Set of vectors closed under addition and scalar mult. | All ML data lives here      |\n",
        "| **Basis**                 | Minimal spanning set                                  | Defines dimensions          |\n",
        "| **Dimension**             | # of basis vectors                                    | Complexity of data          |\n",
        "| **Linear Transformation** | Map vectors via matrices                              | Neural layers, embeddings   |\n",
        "| **Kernel**                | Vectors mapped to zero                                | Dimensionality loss check   |\n",
        "| **Image**                 | Output of transformation                              | Useful in PCA & activations |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **8. Linear Algebra: Orthogonality and Orthonormal Basis**\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 1. **Orthogonality**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "Two vectors `u` and `v` are **orthogonal** if their **dot product is zero**:\n",
        "\n",
        "$$\n",
        "u \\cdot v = 0\n",
        "$$\n",
        "\n",
        "This means the vectors are **perpendicular** in space.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Why Important?**\n",
        "\n",
        "Orthogonal vectors are **linearly independent** ‚Üí no overlap in information.\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ **Example**:\n",
        "\n",
        "$$\n",
        "u = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad\n",
        "v = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\Rightarrow u \\cdot v = 0\n",
        "$$\n",
        "\n",
        "‚Üí Vectors are **orthogonal** (90¬∞ apart)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Properties of Orthogonal Vectors\n",
        "\n",
        "| Property                                  | Description                  |\n",
        "| ----------------------------------------- | ---------------------------- |\n",
        "| `u ‚Ä¢ v = 0`                               | Vectors are perpendicular    |\n",
        "| `‚Äñu + v‚Äñ¬≤ = ‚Äñu‚Äñ¬≤ + ‚Äñv‚Äñ¬≤`                  | **Pythagoras theorem** holds |\n",
        "| Orthogonal vectors ‚áí Linearly independent | No redundancy                |\n",
        "| Used in QR, PCA, SVD                      | Computational stability      |\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ **Orthogonality in AI/ML**\n",
        "\n",
        "| Task                 | Use of Orthogonality                                           |\n",
        "| -------------------- | -------------------------------------------------------------- |\n",
        "| **PCA**              | Principal components are orthogonal directions of max variance |\n",
        "| **Neural Nets**      | Orthogonal initialization of weights improves convergence      |\n",
        "| **Word Embeddings**  | Orthogonal directions = unrelated concepts                     |\n",
        "| **Clustering**       | Orthogonal distance metrics in K-means / cosine similarity     |\n",
        "| **Gradient Descent** | Orthogonal gradients reduce interference during updates        |\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 2. **Orthonormal Vectors**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "\n",
        "Vectors are **orthonormal** if they are:\n",
        "\n",
        "1. **Orthogonal** (perpendicular)\n",
        "2. Each has **unit length** (magnitude = 1)\n",
        "\n",
        "$$\n",
        "u \\cdot v = 0,\\quad ‚Äñu‚Äñ = 1,\\quad ‚Äñv‚Äñ = 1\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ **Example**:\n",
        "\n",
        "$$\n",
        "e‚ÇÅ = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad\n",
        "e‚ÇÇ = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
        "\\Rightarrow \\text{Orthonormal Basis of ‚Ñù¬≤}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Properties of Orthonormal Vectors\n",
        "\n",
        "| Property           | Meaning                                                              |\n",
        "| ------------------ | -------------------------------------------------------------------- |\n",
        "| `u ‚Ä¢ u = 1`        | Unit length                                                          |\n",
        "| `u ‚Ä¢ v = 0`        | Perpendicular                                                        |\n",
        "| **Easy to invert** | If matrix has orthonormal columns, its inverse is its **transpose**: |\n",
        "\n",
        "$$\n",
        "Q^{-1} = Q^T\n",
        "\\] |\n",
        "| **Efficient projections** | Easier computation in ML tasks |\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 3. **Orthonormal Basis**\n",
        "\n",
        "### ‚úÖ **Definition**:\n",
        "An **orthonormal basis** is a set of **orthonormal vectors** that **span a vector space**.\n",
        "\n",
        "So any vector in the space can be written as a **linear combination** of those basis vectors.\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ Example in ‚Ñù¬≥:\n",
        "\\[\n",
        "e‚ÇÅ = [1, 0, 0],\\quad\n",
        "e‚ÇÇ = [0, 1, 0],\\quad\n",
        "e‚ÇÉ = [0, 0, 1]\n",
        "$$\n",
        "\n",
        "These form the **standard orthonormal basis** for ‚Ñù¬≥.\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ **Applications in AI/ML**\n",
        "\n",
        "| Application                            | Description                                                                |\n",
        "| -------------------------------------- | -------------------------------------------------------------------------- |\n",
        "| **PCA**                                | Eigenvectors form an **orthonormal basis** of the data space               |\n",
        "| **Autoencoders**                       | Latent representations aim for orthogonality to **reduce feature overlap** |\n",
        "| **SVD (Singular Value Decomposition)** | Decomposes matrix into orthonormal matrices (U, V)                         |\n",
        "| **QR Decomposition**                   | Used in linear regression, provides orthonormal basis                      |\n",
        "| **Orthogonal Initialization**          | Helps stabilize deep learning training                                     |\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Fast Projection using Orthonormal Basis\n",
        "\n",
        "If `U = [u‚ÇÅ, u‚ÇÇ, ..., u‚Çñ]` are orthonormal, then:\n",
        "\n",
        "To project `x` onto this space:\n",
        "\n",
        "$$\n",
        "\\text{proj}_{U}(x) = \\sum_{i=1}^{k} (x \\cdot u·µ¢) u·µ¢\n",
        "$$\n",
        "\n",
        "This makes dimensionality reduction (like PCA) **computationally cheap** and **interpretable**.\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 4. **Gram-Schmidt Process** (To create orthonormal basis)\n",
        "\n",
        "Given a set of vectors, the **Gram-Schmidt algorithm** constructs an orthonormal basis.\n",
        "\n",
        "### ‚ú≥Ô∏è Python Snippet:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def gram_schmidt(V):\n",
        "    U = []\n",
        "    for v in V:\n",
        "        for u in U:\n",
        "            v = v - np.dot(v, u) * u\n",
        "        U.append(v / np.linalg.norm(v))\n",
        "    return np.array(U)\n",
        "\n",
        "V = np.array([[1, 1], [1, 0]])\n",
        "orthonormal_basis = gram_schmidt(V)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Flash Summary\n",
        "\n",
        "| Concept           | Meaning                                                   | Use in ML                         |\n",
        "| ----------------- | --------------------------------------------------------- | --------------------------------- |\n",
        "| Orthogonal        | Vectors at 90¬∞                                            | Feature independence              |\n",
        "| Orthonormal       | Orthogonal + Unit Length                                  | Basis in PCA, SVD                 |\n",
        "| Orthonormal Basis | Minimal, stable basis                                     | Efficient vector projections      |\n",
        "| Gram-Schmidt      | Turns linearly independent vectors into orthonormal basis | Preprocessing for PCA, regression |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "C_Z3QKQk6bDO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bxrzdqn6R8h"
      },
      "outputs": [],
      "source": []
    }
  ]
}